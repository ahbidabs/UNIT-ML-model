{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksGk6xWoBxIn",
        "outputId": "5139cecf-3507-41cb-b168-4e35e9fa438b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/sensormovingshit.zip\n",
            "   creating: /content/50hz/sensormovingshit/\n",
            "  inflating: /content/50hz/sensormovingshit/0 reps (still).txt  \n",
            "  inflating: /content/50hz/__MACOSX/sensormovingshit/._0 reps (still).txt  \n",
            "  inflating: /content/50hz/sensormovingshit/lat pulldown 3.txt  \n",
            "  inflating: /content/50hz/__MACOSX/sensormovingshit/._lat pulldown 3.txt  \n",
            "  inflating: /content/50hz/sensormovingshit/lat pulldown 4.txt  \n",
            "  inflating: /content/50hz/__MACOSX/sensormovingshit/._lat pulldown 4.txt  \n",
            "  inflating: /content/50hz/sensormovingshit/lat pulldown 5.txt  \n",
            "  inflating: /content/50hz/__MACOSX/sensormovingshit/._lat pulldown 5.txt  \n",
            "  inflating: /content/50hz/sensormovingshit/0 reps (swinging).txt  \n",
            "  inflating: /content/50hz/__MACOSX/sensormovingshit/._0 reps (swinging).txt  \n",
            "  inflating: /content/50hz/sensormovingshit/lat pulldown.txt  \n",
            "  inflating: /content/50hz/__MACOSX/sensormovingshit/._lat pulldown.txt  \n",
            "  inflating: /content/50hz/sensormovingshit/0 reps (random horizonal movement).txt  \n",
            "  inflating: /content/50hz/__MACOSX/sensormovingshit/._0 reps (random horizonal movement).txt  \n",
            "  inflating: /content/50hz/sensormovingshit/0 reps(still).txt  \n",
            "  inflating: /content/50hz/__MACOSX/sensormovingshit/._0 reps(still).txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/sensormovingshit.zip\" -d \"/content/50hz\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Keywords indicating no exercise sessions\n",
        "NO_EXERCISE_KEYWORDS = ['no exercise', '0 reps', 'setup', 'failed', 'random']\n",
        "\n",
        "# Only one exercise: lat pulldown\n",
        "EXERCISE_KEYWORDS = {\n",
        "    'lat_pulldown': ['lat pulldown']\n",
        "}\n",
        "\n",
        "# File paths for saving models\n",
        "MODEL_DIR = '/content/50hz/models'\n",
        "CLASSIFIER_PATH = os.path.join(MODEL_DIR, 'rf_classifier.joblib')\n",
        "ENCODER_PATH = os.path.join(MODEL_DIR, 'label_encoder.joblib')\n",
        "SCALER_PATH = os.path.join(MODEL_DIR, 'scaler.joblib')\n",
        "\n",
        "\n",
        "def ensure_model_dir():\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def save_models(clf, le, scaler):\n",
        "    ensure_model_dir()\n",
        "    joblib.dump(clf, CLASSIFIER_PATH)\n",
        "    joblib.dump(le, ENCODER_PATH)\n",
        "    joblib.dump(scaler, SCALER_PATH)\n",
        "    print(f\"Saved classifier to {CLASSIFIER_PATH}\")\n",
        "    print(f\"Saved encoder to {ENCODER_PATH}\")\n",
        "    print(f\"Saved scaler to {SCALER_PATH}\")\n",
        "\n",
        "\n",
        "def load_models():\n",
        "    clf = joblib.load(CLASSIFIER_PATH)\n",
        "    le = joblib.load(ENCODER_PATH)\n",
        "    scaler = joblib.load(SCALER_PATH)\n",
        "    return clf, le, scaler\n",
        "\n",
        "\n",
        "def load_and_label_data(data_dir, file_pattern=\"*.txt\"):\n",
        "    \"\"\"\n",
        "    Load txt files, label as 'lat_pulldown' or 'no_exercise'\n",
        "    based on filename keywords.\n",
        "    \"\"\"\n",
        "    all_files = glob.glob(os.path.join(data_dir, file_pattern))\n",
        "    data_list, labels = [], []\n",
        "    for f in all_files:\n",
        "        name = os.path.basename(f).lower()\n",
        "        # Check no exercise first\n",
        "        if any(kw in name for kw in NO_EXERCISE_KEYWORDS):\n",
        "            lbl = 'no_exercise'\n",
        "        # Then lat pulldown\n",
        "        elif any(kw in name for kw in EXERCISE_KEYWORDS['lat_pulldown']):\n",
        "            lbl = 'lat_pulldown'\n",
        "        else:\n",
        "            # Skip any other files\n",
        "            continue\n",
        "\n",
        "        # Read file\n",
        "        df = pd.read_csv(f, sep='\\t', parse_dates=['time'], engine='python')\n",
        "        # Drop unwanted columns\n",
        "        drop_cols = ['time', 'DeviceName', 'Version()', 'Temperature(°C)', 'Battery level(%)']\n",
        "        df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "        data_list.append(df)\n",
        "        labels.append(lbl)\n",
        "\n",
        "    return data_list, labels\n",
        "\n",
        "\n",
        "def extract_features(df, window_size=25, step_size=10):\n",
        "    \"\"\"\n",
        "    Sliding-window feature extraction: mean, std, min, max of numeric columns.\n",
        "    \"\"\"\n",
        "    num_df = df.select_dtypes(include=[np.number])\n",
        "    windows = []\n",
        "    for start in range(0, len(num_df) - window_size + 1, step_size):\n",
        "        w = num_df.iloc[start:start + window_size]\n",
        "        feat = {}\n",
        "        for col in num_df.columns:\n",
        "            feat[f\"{col}_mean\"] = w[col].mean()\n",
        "            feat[f\"{col}_std\"] = w[col].std()\n",
        "            feat[f\"{col}_min\"] = w[col].min()\n",
        "            feat[f\"{col}_max\"] = w[col].max()\n",
        "        windows.append(feat)\n",
        "    return pd.DataFrame(windows)\n",
        "\n",
        "\n",
        "def build_dataset(data_list, labels, window_size=25, step_size=10):\n",
        "    X_parts, y_parts = [], []\n",
        "    for df, lbl in zip(data_list, labels):\n",
        "        feats = extract_features(df, window_size, step_size)\n",
        "        if feats.empty:\n",
        "            continue\n",
        "        X_parts.append(feats)\n",
        "        y_parts.extend([lbl] * len(feats))\n",
        "    X = pd.concat(X_parts, ignore_index=True)\n",
        "    y = np.array(y_parts)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def train_classifier(X, y, save=True):\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y)\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
        "    )\n",
        "    # Train\n",
        "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "    if save:\n",
        "        save_models(clf, le, scaler)\n",
        "    return clf, le, scaler\n",
        "\n",
        "\n",
        "def predict_exercise(clf, le, scaler, df, window_size=25, step_size=10):\n",
        "    # Prepare df\n",
        "    df_proc = df.copy()\n",
        "    if 'time' in df_proc.columns:\n",
        "        df_proc = df_proc.drop('time', axis=1)\n",
        "    # Feature extraction\n",
        "    feats = extract_features(df_proc.select_dtypes(include=[np.number]), window_size, step_size)\n",
        "    X_scaled = scaler.transform(feats)\n",
        "    preds = clf.predict(X_scaled)\n",
        "    # Return most frequent\n",
        "    common = np.bincount(preds)\n",
        "    top = common.argmax()\n",
        "    return le.inverse_transform([top])[0]"
      ],
      "metadata": {
        "id": "DHR2pTPGB93n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Example Usage =====\n",
        "if __name__ == '__main__':\n",
        "    data_dir = '/content/50hz/sensormovingshit'\n",
        "    # 1) Train and save model\n",
        "    data_list, labels = load_and_label_data(data_dir)\n",
        "    X, y = build_dataset(data_list, labels)\n",
        "    train_classifier(X, y)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPLlV43wB-72",
        "outputId": "bae50800-6429-40f2-9c9a-f224295d7b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "lat_pulldown       1.00      1.00      1.00        20\n",
            " no_exercise       1.00      1.00      1.00        37\n",
            "\n",
            "    accuracy                           1.00        57\n",
            "   macro avg       1.00      1.00      1.00        57\n",
            "weighted avg       1.00      1.00      1.00        57\n",
            "\n",
            "Saved classifier to /content/50hz/models/rf_classifier.joblib\n",
            "Saved encoder to /content/50hz/models/label_encoder.joblib\n",
            "Saved scaler to /content/50hz/models/scaler.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Inference\n",
        "clf, le, scaler = load_models()\n",
        "test_file = '/content/lat pulldown 5.txt'\n",
        "df_test = pd.read_csv(test_file, sep='\\t', parse_dates=['time'], engine='python')\n",
        "drop_cols = ['DeviceName', 'Version()', 'Temperature(°C)', 'Battery level(%)']\n",
        "df_test = df_test.drop(columns=[c for c in drop_cols if c in df_test.columns], errors='ignore')\n",
        "predicted = predict_exercise(clf, le, scaler, df_test)\n",
        "print(f\"Predicted: {predicted}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "xSYZY3TjCE-1",
        "outputId": "46ea1a3f-6ead-4f60-e5ce-772a50dc5255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/lat pulldown 5.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ebd817262fe5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/lat pulldown 5.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdrop_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'DeviceName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Version()'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Temperature(°C)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Battery level(%)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdrop_cols\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/lat pulldown 5.txt'"
          ]
        }
      ]
    }
  ]
}